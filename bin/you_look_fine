#!/usr/bin/env python3

import argparse
import cv2
import os
import time
import sys

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# Parse command-line arguments
parser = argparse.ArgumentParser(description='Run YouLookFine.')
parser.add_argument('--motion-threshold', type=int, default=10000, help='The motion detection threshold.')
parser.add_argument('--max-time-between-requests', type=int, default=500000, help='The maximum time between requests, milliseconds.')
parser.add_argument('--debug', action='store_true', help='Enable debug mode.')
parser.add_argument('--mute', action='store_true', help='Mute audio.')
parser.add_argument('--run-once', action='store_true', help='Runs until detection is made and compliment is given.')
parser.add_argument('--vision-prompt', type=str, default="Give a short compliment based on the person's appearance in the image. Call out distinct features.", help='Instructions for vision.')
parser.add_argument('--voice', type=str, default='fable', choices=['alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer'], help='The voice to use for text-to-speech.')
parser.add_argument('--infer-action', action='store_true', help='Infer action in the scene from multiple frames.')
parser.add_argument('--capture-frames', type=int, default=1, help='The number of frames to capture when inferring action.')
parser.add_argument('--capture-frames-delay', type=int, default=1000, help='The delay between capturing frames, in milliseconds.')
parser.add_argument('--camera', default=0, help='The camera source. Can be an integer for a local camera or a string for an RTSP URL.')
parser.add_argument('--soft-run', action='store_true', help='Run without any calls to APIs.' )

# Setup configuration
from you_look_fine.config import config

config.update(vars(parser.parse_args()))

from you_look_fine.vision import Vision 
from you_look_fine.voice import Voice
from you_look_fine.camera_reader import CameraReader
from you_look_fine.logger import logger

logger.info("Initializing...")
# Initialize the motion detection
camera_source = config['camera']
try:
    # Try to convert the camera source to an integer
    camera_source = int(camera_source)
except ValueError:
    # If it can't be converted to an integer, assume it's an RTSP URL
    pass

camera = cv2.VideoCapture(camera_source)
motion_detector = cv2.createBackgroundSubtractorMOG2()

# Initialize the request state
frame_counter = 0
request_in_progress = False
camera_warmed_up = False

WARM_UP_FRAMES = 10
frames = []

voice = Voice('/tmp/you_look_fine_response.mp3')
vision = Vision()
reader = CameraReader(camera)
reader.start()

try:
    while True:
        frame = reader.get_latest_frame()
        if frame is None:
            continue
        # Apply the motion detector to the frame
        motion_mask = motion_detector.apply(frame)

        if not camera_warmed_up:
            frame_counter += 1
            if frame_counter > WARM_UP_FRAMES:
                # After 20 frames, consider the camera warmed up
                logger.info("Camera initialized")
                camera_warmed_up = True
            else:
                continue

        motion_detected = motion_mask.sum() > config['motion_threshold'] 
        time_since_last_vision_request = time.time() - vision.last_request['time']
        can_make_requests = time_since_last_vision_request > (config['max_time_between_requests']/ 1000)

        # If there's enough motion and no request is in progress, proceed
        if motion_detected and can_make_requests and not request_in_progress:
            logger.info("Motion detected")

            for _ in range(config['capture_frames']):
                frame = reader.get_latest_frame()  # Capture a new frame
                if frame is None:
                    continue

                image_path = f'/tmp/detected_motion_{len(frames)}.jpg'
                cv2.imwrite(image_path, frame)
                frames.append(image_path)

                logger.debug("Frame captured, waiting %d ms", config['capture_frames_delay'])

                time.sleep(config['capture_frames_delay'] / 1000)  # convert ms to seconds

            # If we have enough frames or we're not inferring action, proceed
            frame_buffer_ready = len(frames) == config['capture_frames']

            if frame_buffer_ready:
                logger.debug("Making vision request: %s", frames)

                if not config['soft_run']:
                    # Get vision request for the images
                    request_in_progress = True
                    
                    vision_response = vision.analyze(frames, config['vision_prompt'])
                    logger.debug("Vision Response: %s", vision_response)
                    # Play the audio file
                    if not config['mute']:
                        # Convert the vision request to speech
                        logger.debug("%s speaking...", config['voice'])
                        voice.text_to_speech(vision_response, config['voice'])
                        voice.speak()

                    # Set the request state back to not in progress and clear the frames
                    frames = []
                    request_in_progress = False

                if config['run_once']:
                    break

except KeyboardInterrupt:
    logger.info("Interrupted by user. Exiting...")
finally:
    # Release the camera and destroy all windows
    reader.stop()
    time.sleep(0.1)
    camera.release()
    cv2.destroyAllWindows()