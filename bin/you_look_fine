#!/usr/bin/env python3

import argparse
import cv2
import os
import time
import sys
import numpy as np

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# Parse command-line arguments
parser = argparse.ArgumentParser(description='Run YouLookFine.')
parser.add_argument('--motion-threshold', type=int, default=10000, help='The motion detection threshold.')
parser.add_argument('--max-time-between-requests', type=int, default=500000, help='The maximum time between requests, milliseconds.')
parser.add_argument('--debug', action='store_true', help='Enable debug mode.')
parser.add_argument('--mute', action='store_true', help='Mute audio.')
parser.add_argument('--run-once', action='store_true', help='Runs until detection is made and compliment is given.')
parser.add_argument('--vision-prompt', type=str, default="Give a short compliment based on the person's appearance in the image. Call out distinct features.", help='Instructions for vision.')
parser.add_argument('--voice', type=str, default='fable', choices=['alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer'], help='The voice to use for text-to-speech.')
parser.add_argument('--infer-action', action='store_true', help='Infer action in the scene from multiple frames.')
parser.add_argument('--capture-frames', type=int, default=1, help='The number of frames to capture when inferring action.')
parser.add_argument('--capture-frames-delay', type=int, default=1000, help='The delay between capturing frames, in milliseconds.')
parser.add_argument('--camera', default=0, help='The camera source. Can be an integer for a local camera or a string for an RTSP URL.')
parser.add_argument('--soft-run', action='store_true', help='Run without any calls to APIs.' )
parser.add_argument('--detect', nargs='+', help='The features to detect.', default=['person'], choices=['person', 'car', 'cat', 'dog', 'bird', 'horse', 'sheep', 'cow'])

# Setup configuration
from you_look_fine.config import config

config.update(vars(parser.parse_args()))

from you_look_fine.vision import Vision 
from you_look_fine.voice import Voice
from you_look_fine.camera_reader import CameraReader
from you_look_fine.logger import logger

logger.info("Initializing...")

# Initialize the request state
frame_counter = 0
request_in_progress = False

WARM_UP_FRAMES = 100
frames = []

voice = Voice('/tmp/you_look_fine_response.mp3')
vision = Vision()
vision.detail = 'low'
reader = CameraReader( config['camera'], config['motion_threshold'], config['debug'])
reader.start()

try:
    while True:
        frame = reader.get_latest_frame()
        if frame is None:
            continue

        if frame_counter <= WARM_UP_FRAMES:
            frame_counter += 1
            continue

        if config['debug']:
            cv2.imshow('Main Feed', frame)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

        time_since_last_vision_request = time.time() - vision.last_request['time']
        can_make_requests = time_since_last_vision_request > (config['max_time_between_requests']/ 1000)

        # If there's enough motion and no request is in progress, proceed
        if reader.motion_detected() and can_make_requests and not request_in_progress:

            if config['debug']:
                logger.debug("Motion detected: %s, %s", reader.motion_mask.sum(), config['motion_threshold'])
                # Display the motion mask
                cv2.imshow('Motion Mask', reader.motion_mask)
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break

                detection_overlay = reader.feature_detection_frame()
                if detection_overlay is not None:
                    cv2.imshow('Features', detection_overlay)
                    if cv2.waitKey(1) & 0xFF == ord('q'):
                        break

            if reader.has_feature(*config['detect']):

                logger.info("Feature detected: %s", reader.detected_features)
                frames = reader.capture_frames(config['capture_frames'], config['capture_frames_delay'])
                # If we have enough frames or we're not inferring action, proceed
                if len(frames) == config['capture_frames']:
                    logger.info("Making vision request: %s", frames)

                    if not config['soft_run']:
                        # Get vision request for the images
                        request_in_progress = True
                        
                        vision_response = vision.analyze(frames, config['vision_prompt'])
                        logger.info("Vision Response: %s", vision_response)
                        # Play the audio file
                        if not config['mute']:
                            # Convert the vision request to speech
                            logger.info("%s speaking...", config['voice'])
                            voice.text_to_speech(vision_response, config['voice'])
                            voice.speak()

                        # Set the request state back to not in progress and clear the frames
                        request_in_progress = False
                    else: 
                        logger.debug("Soft run, not making request. time_since_last_vision_request: %s", time_since_last_vision_request)
                        vision.last_request = {'time': time.time()}

                    if config['run_once']:
                        break

except KeyboardInterrupt:
    logger.info("Interrupted by user. Exiting...")
finally:
    # Release the camera and destroy all windows
    reader.stop()